\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 1: \emph{Single Agent Planning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}
In this assignment, we study the application of dynamic programming algorithms to find a best policy for Markov Decision Processes (MDP's). The MDP we implement is the \emph{predator versus prey grid world}. A predator and a prey move around a field, in which the predator wants to catch the prey. Our goal is to find the best poliy for the predator to catch the prey. We evaluate the following methods:
\begin{itemize}
\item a random policy
\item Policy Evaluation
\item Policy Iteration
\item Value Iteration
\end{itemize}
\section{Program design}
Our program has been written in Java. The algorithm for every policy can be started from the corresponding main class: \emph{Main}, \emph{MainPE}, \emph{MainPI} and \emph{MainVI}. The main class creates an environment (specific for a certain policy), in which the predator and the prey reside. Then, it runs the simulation for that environment. 

The grid location of the predator/prey is stored in the predator/prey object itself. There is no explicit grid object of which the predator and the prey are part. Using a subclass structure from \emph{Predator}, there are different predator objects for the different policies. The prey always has the same policy: it stays on the same place with probability $0.8$ and moves in one of the 4 directions with probability $0.05$.

The predator internally stores a 4-dimensional values array. The values of all states, calculated by the MDP policy algorithms, are stored in this array. The array is 4-dimensional because every state is a tuple $(predator_x,predator_y,prey_x,prey_y)$.
\section{Algorithms}
\subsection{Random policy}
Using the random policy, the predator moves in every direction (4 directions or stay on same place) with probability $0.2$. Running the simulation 100 times and measuring the time it takes to catch the prey, we get the following results:

\begin{tabular}{l|l}
Run&Number of iterations\\
\hline
1&291\\
2&36\\
3&31\\
4&176\\
5&304\\
6&307\\
7&332\\
8&198\\
9&67\\
10&444\\
11&262\\
12&273\\
13&432\\
14&149\\
15&133\\
16&295\\
17&21\\
18&237\\
19&146\\
20&129\\
21&56\\
22&173\\
23&841\\
24&173\\
25&899\\
26&66\\
27&398\\
28&291\\
29&32\\
30&31\\
31&37\\
32&35\\
33&498\\
34&378\\
35&366\\
36&144\\
37&143\\
38&155\\
39&700\\
40&503\\
41&510\\
42&103\\
43&826\\
44&100\\
45&143\\
46&104\\
47&223\\
48&600\\
49&93\\
50&269\\
51&69\\
\end{tabular}
\begin{tabular}{l|l}
Run&Number of iterations\\
\hline
52&21\\
53&434\\
54&686\\
55&295\\
56&770\\
57&81\\
58&306\\
59&37\\
60&114\\
61&157\\
62&130\\
63&482\\
64&114\\
65&108\\
66&309\\
67&39\\
68&472\\
69&23\\
70&526\\
71&88\\
72&1210\\
73&337\\
74&404\\
75&141\\
76&111\\
77&375\\
78&27\\
79&116\\
80&58\\
81&396\\
82&1496\\
83&219\\
84&47\\
85&1493\\
86&363\\
87&301\\
88&215\\
89&49\\
90&50\\
91&91\\
92&91\\
93&181\\
94&160\\
95&57\\
96&208\\
97&1033\\
98&72\\
99&20\\
100&210\\
\hline
Average&279.45\\
Standard deviation&342.7141666170221\\
\end{tabular}

\subsection{Policy evaluation}
With policy evaluation, we evaluated the values of the states using a random policy. We looked at four different locations for the prey and predator. For these four locations, we calculated the policy evaluation value and measured how many iterations it took the algorithm to a stable value array:

\begin{itemize}
\item \emph{Prey(5,5) Predator(0,0)}

Number of iterations before value matrix converges: 35

Evaluation = 0.008985523068296548

\item \emph{Prey(5,4) Predator(2,3)}

Number of iterations before value matrix converges: 35

Evaluation = 0.28595851469775013

\item \emph{Prey(10,0) Predator(2,10)}

Number of iterations before value matrix converges: 35

Evaluation = 0.28595886049229247

\item Prey(0,0) Predator(10,10)

Number of iterations before value matrix converges: 35

Evaluation = 1.9428943967738175
\end{itemize}

It seems that the highest score is given to settings where the predator is far from the prey.
\subsection{Policy iteration}
We implemented policy iteration, which starts from an arbitrary policy and improves the policy by evaluating it.

Detailed results for discount factors of 0.1, 0.5, 0.7 and 0.9 are available in the \emph{results} folder, file names starting with $PI$.
\subsection{Value iteration}
Compared with value iteration, the time of iterations increases as the $\gamma$ getting closer to 1 for both of the algorithms. The number of iterations in value iteration are more or less the same with that in the first two rounds in policy iteration, given the same $gamma$. However,the policy iteration converged much more fast after the first two rounds.

Detailed results including output values for discount factors of 0.1, 0.5, 0.7 and 0.9 are available in the \emph{results} folder, file names starting with $VI$.
\section{Efficient state-space representation}
The state-space consists of $11^4$ items: both the predator and the prey have coordinates in a $11 \times 11$ space. It would save computation time if the policy algorithms do not have to loop over $11^4$, but less, items.

A solution to this would be to represent the locations of the predator and the prey not as absolute locations, but as relative locations.

In this case, the predator stores a tuple $\delta_{p}(x,y)$ for every prey $p$, which denotes the distance to $p$. $x$ and $y$ are updated by +1 or -1 every time the predator or prey moves. Every iteration, the modulo of $x$ and $y$ is taken, because the environment is a torus:
\begin{align}
x := x\,mod\,11\\
y := y\,mod\,11
\end{align}

\section{Conclusion}
We tried out different policy algorithms for a simple MDP, a race between predator and prey. Using a random policy, the predator catches the prey in an average of 280 iterations. Policy evaluation makes it possible to see when the predator has the best policy. For the random policy, this is when the predator is removed far from the prey. Policy iteration and value iteration are ways to find a good policy for the predator. It seems that a higher discount factor gives a higher number iterations that is needed.

An interesting addition to our program would be to encode the state-space in such a way that the coordinates of the prey are relative to those of the predator. 
\end{document}
