\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 2: \emph{Single Agent Learning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}
In this assignment, we study the application of \emph{reinforced learning
algorithms}. The main focus of this study is on the Q-Learning algorithm where
we used a \epsilon-policy at the begining and changed other policies to experiment
the efects of that.

Our goal is to tune the algorithms with the different parameters and understand
how this parameters affect the performance that in this specific instance is
determined by the number of steps the predator needs to catch the prey and how
we can improve that with learning. 

\section{Program design}
In this assignment the overall progam design stays the same, as we just added
the equivalent classes for the new algorithms. So we have a new main class,
\emph{MainQl} that, like the others, create a specific environment for the
predator and prey to live. Then it runs the simulation for that setup.

We also added some utility classes to produce the graphs for this report.

\section{Evaluation}

%- Why? 'In order to test\ldots'
%- How? ' we run \ldots N times on X computer'
%- What does it show?
%- Take home message

In order to test how the Q-learning algorithm works with different parameters we
have run our implemetation and computed the average of $100$ runs, each of them
with $10000$ episodes. With the averaging we have been able to get less sharper
graphs.

\subsection{Different learning rates (\alpha)}
We started to experiment with different values of \alpha. The values used are:
$0.1 , 0.2, 0.3, 0.4 and 0.5$. 

We have noticed that, as expected, when we increse the value of \alpha algorithm
converges faster to an optimal value. This happens because this makes the agent
consider the the known information. If we choose $0$ for \alpha the agent would
not learn anything as he would just use the old(inital) values.
In theory we could put it to $1$ to maximise de learning rate, but as our
transitions are not deterministic, because the agent doesn't control the
movement of the prey we can't trust only on learning.
Thus, we should set the values as high as possible but considering this tradeof.
So, as said we used the values from $0.1, \ldots , 0.5$ test and the results are
better with $0.5$ as the chart shows. They all converge but with higher values
they converge faster. The average value shows how fast they converge, because if
a line converge faster, the number of low values will be high and this have a
direct impact on the average.

\includegraphics[]{alpha_01_to_05_gama_09_epsilon_01_IV_15}

\subsection{Different discount factors (\gamma)}
We started to experiment with different values of \gamma. The values used are:
$0.2, 0.5, 0.7 and 0.9$. 

The discount factor influences the importance that is given to future rewards.
This means that the lower the value, the less the agent cares about future
rewards and considers more the imediate reward. In the limit, if it's set to
$0$ (zero), the agent only considers the imediate reward.

In this problem instance the imediate reward is almost allways zero, so if we
have low \gamma the learned values will be close to zero, i. e., it will not
learn anything. 

Thus, we noticed that by incresing \gamma the number of steps converge to lower
values but don't have a big impact on the convegence rate.

\includegraphics[]{alpha_04_gama_02_to_09_epsilon_01_IV_15}

The chart shows what we just mentioned, that with higher values for \gamma, the
line converges to a lower point.

\subsection{Different Initial Values}
We started to experiment with different values of IV. The values used are:
$0, 1, 15 and 50$. 

As Q-learning is an iterative process, and we assign initial values for the
states, we have to consider what values we use and the impact it may have on the
performance/evolution of the process.

It have been shown that high values for the initialization, aka \"optimistic
initial conditions\", can encourage exploration. This is positive because after
the agent explores all the states and assign the values for that states, in can
start optimizing paths.
With low initial values, consequently low exploration, the number of states that
still have the initial values are higher through the process and when the agent
is surrounded by states with the same value, it chooses randomly. This causes a
behaviour closer to a random policy at the begining cousing the first group of
iterations to have much higher number of steps than with \"optimistic
initial conditions\".

\includegraphics[]{res\alpha_04_gama_02_to_09_epsilon_01_IV_15}

\subsection{Different \epsilon's}
We started to experiment with different values of \epsilon. The values used are:
$0.1, 0.3, 0.5 and 0.7$.

The 

\includegraphics[]{alpha_04_gama_02_to_09_epsilon_01_IV_15}

\section{Conclusion}


\end{document}
