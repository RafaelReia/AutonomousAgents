\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 3: \emph{Multi-agent planning and learning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}
In this assignment, we study the application of \emph{multi-agent learning}. To go from a single-agent situation to a multi-agent situation for our predator-prey scenario, we study two perspectives:

\begin{itemize}
 \item In \emph{Independent Q-learning}, one or more predators try to catch a prey, which is also a rational agent. The agents apply Q-learning independently of eachother.
 \item In \emph{Markov games with the minimax-Q algorithm}, one predator and one prey compete with eachother. Both are agents which try to optimize their policies by applying the minimax algorithm, maximizing their own policy for the worst possible policy by the opponent.
\end{itemize}
\section{Multi-predator random policy}
The first step for this assignment was to extend our environment to be able to
handle multiple agents. The environment can now contain more than one predator and a prey that also acts as a learning agent.
We did not change much of our implementation, because most of our code had
already been prepared for the changes.

We didn't change much of our implementation because most of our code was
already prepared to this changes. We runed some tests to use as a baseline for
the evaluation of the following evironments.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
Number of predators    & 1       & 2       & 3       & 4      \\ \hline
Average steps of caugh & 217.144 & 80.977  & 45.248  & 31.203 \\ \hline
Standard Dev of caugh  & 199.497 & 63.250  & 34.917  & 18.432 \\ \hline
Average steps of clash &         & 41.535  & 13.659  & 5.082  \\ \hline
Standard Dev of clash  &         & 58.688  & 24.379  & 9.112  \\ \hline
\% Caught              & 100\%   & 44.40\% & 18.90\% & 5.40\% \\ \hline
\end{tabular}
\end{table}

\section{State-space reduction}
Although, in this process we noticed that our state representation would grow
(exponentially) too much with the addition of more predators. Represented the
state space as the \emph{coordinates} of all individuals, which is implemented as
an array with $2 \times (n+1)$ dimensions, with the $n$ as the number of
predators. So with four predators we had a $10-D$ array with each dimension
with the value $11$.

To reduce the number of dimensions we implemented \emph{relative positions},
that represent the distance of each predator to the prey. With this we reduced the
state space by two dimensions. This does not look a lot at first sight, but it makes the 3-predator situation solvable on our machines. It also makes the algorithm converge much faster on 1 or predator settings. Taking the 1-predator situation as an example, with the original state-space, the algorithm will need to wait until the action-value function converge on $121^2$ states; but now it only need to converge on $121$ states.

To reduce even more of the state space, we wanted to represent the states as
\emph{(direction, square distance)} for each predator. With this modification, we have the
same number of dimensions, but the size of the direction dimension is reduced to $4$.
This would have a significant impact on the size of the state space reducing it
by a factor of $4\cdot10^4$. After that we reduce more the size of the state space with
two different options. The first was by setting the distance as $5$ if it was
higher than that and the second by setting the distance as the minimum of both
directions instead of the sum.
With this, we reduce the amount of information that we keep but we turn this into
a tractable problem with four predators.

It turned out we could not further reduce the encoding of the state to just the \emph{square distance} to the prey. That
would reduce the dimensions to just one for each predator. We would then loose important information for the agents: direction is important to run away
when close.

\section{Multi-Agent independent Q-learning}
We implemented \emph{multi-agent independent Q-learning} and applied it to the predators and preys. The prey does not have a random policy anymore, but acts as an agent. The agents learn
independently of each other, this means that each agent is trying to optimize
its own rewards. The agents have their own Q-value arrays and receive their own rewards.

\subsection{Evaluation}
To evaluate our implementation of multi-agent independent Q-learning, we tested
with one, two and three predators. We did not test with more because the state
space would be intractably large with the available computing resources. In the
following subsections we will explain the three cases separately.

One big difference exists between the case with only one predator and the cases with
more than one predator. With only one predator, the prey can run away forever, because it pursuits the same smart policy (with opposite rewards) as the predator. To prevent this, the prey has a trip factor that causes the
prey to wait $\eta\%$ of the times.

\subsection{One predator}
As briefly explained, the behaviour for this setting is different from the
following ones. The predator always catches the prey, similar to the previous
assignments. To evaluate this behaviour, we use the number of steps needed to
catch the prey. With this metric, we want to evaluate how the prey learns, compared to
the predator. Although it has a trip probability, the prey would still manage to
stay alive longer if it learns faster.

We run this environment with $1000$ episodes, we did not use more because the
state space is small and is covered very fast. This number of episodes also converges quickly.

The result that we have is in the following figures.

We tested a setting with both smart agents. So we used the best
values for the parameters for both agents. With this we wanted to show the effect
of the trip, because with the same parameters the agents learn at the same rate.

The only difference between the agents is the trip factor for the prey.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{res/1_both_smart.png}
    \caption{One smart predator(0.5, 0.9, 0, 15), one smart prey(0.5, 0.9, 0, 15)}
    \label{fig:fig2}
\end{figure}


The next step was to test the same settings as before, but with a purely greedy
policy for both agents, that means, setting the $\epsilon$ to zero. Because we
have an optimistic initialisation, the agents still go to unexplored places
without the random move factor.
\error explain results missing

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{res/1_both_smart_e01.png}
    \caption{One smart predator(0.5, 0.9, 0.1, 15), one smart prey(0.5, 0.9, 0.1, 15)}
\end{figure}

With both agents learning fast, i.e. two smart agents, we can see on the last to
figures that it converges really fast but to a value sligtly higher than the
previous ones. This is because the prey is also learning.
We can see a difference between this to figures, the first with some random
moves ($\epsilon$ set to $0.1$) the first episodes have a smaller number of
steps than the other one. This happens becasue in the second case the agent
would always go to new states, exploring more in the first episodes and reduce
very fast the number of steps.
The spike in the figure \ref{fig2} can be explained by the following, the agent
found a set of states that wasn't explored yet.

After this we wanted to test if we could have a smart prey that manages to never get caught and a dumb predator. Because we still have the trip
factor, we are just testing how long a prey manages to stay alive, and whether this
number grows with learning.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{res/1_dumb_predator.png}
    \caption{One dumb predator(0.01, 0.1, 0.1, 0), one smart prey(0.5, 0.9, 0, 15)}
\end{figure}

%\error explain results missing


\subsection{Two Predators}
\label{twoP}
With more than one predator we add a new possible behaviour to the environment. There is the possiblity that the predators clash into eachother and the prey runs away. The
probability of this behaviour is very high at the beginning if the
predators start at close positions. To evaluate this new environment, the number
of steps is not enough, because they predators can clash into eachother instead of catching the prey.

Now, we have to compare the ratio between the number of times that the prey
is caught versus the number of times that it escapes. Together 
with the number of steps, we can have a much better understanding of what is
happening in our simulation.

We run this environment with $100000$ episodes. Since the state space grows
very fast with the number of agents, we need to let the number of episodes grow
accordingly, to make sure that a higher rate of states is explored.

At first we wanted to test how agents with the same parameters, therefore
the same learning rates would behave in our environment. If we could reduce the
number of clashes, i.e. the predators learn to not kill each other and catch the
prey and how the prey would run away from a smart group of predators.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_both_smart_greedy.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_both_smart_greedy_100.png}
	\end{minipage}
\end{figure}


As we can see, at the begining the predators clash a lot, so the ratio of
catch/clash is very low. But it looks like they are learning to not kill each
other. After some time, the clashes almost goes to zero and then they start
optimizing the catches. We can clearly see that after this phase the number of
steps converges fast.


At second we tested two agents with the same parametes like before but the value
for $\epsilon$ is zero, so we are using a pure greedy policy.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_both_smart_e01.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_both_smart_e01_100.png}
	\end{minipage}
\end{figure}

Again with two agents learning at the same rate, we changed the only the
$\epsilon$ from the last figure. We can see the same pattern than before with
just one predator. It first goes very high and then goes down very fast.

After that, we tested how long a smart prey would live against less smart
predators. In order to do this, we used the same parameters from the last section, but a purely greedy policy for the prey.
With this, we also test if the clashing is a real problem and if it is being solved
through learning. This would cause the predators to clash more with less
learning. 

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_dumb_predator_e01_greedy.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/2_dumb_predator_e01_greedy_100.png}
	\end{minipage}
\end{figure}

%\error explain results missing


\subsection{Three Predators}
We added another predator to the environment explained in  section \ref{twoP}.
With more predators, it should be easier and faster to catch the prey. On the
other hand, the probability of clashing increases. Theoretically, predators put
less effort to catch the prey and more to avoid clashes.

We also used the same evaluation model as in the last section.

We ran this environment with $500000$ episodes, because, as the size of the state
space grows very fast, the time that is needed to test this environments grows.
We could not test with a bigger number of episodes, because it would turn out to
be impractical. However, we still tested with a higher number of episodes, but not
in a systematic way.

Again, we have the same testing goals as in the last section.  With both agents being smart, we wanted to test how the prey would react with more predators and if they would
clash more or catch the prey faster.
\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/3_both_smart_e01_greedy.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/3_both_smart_e01_greedy_100.png}
	\end{minipage}
\end{figure}
%\error explain results missing

After that we again tested with dumb predators and a smart pure greedy prey to
know how it would survive with one more predator.
\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/3_dumb_predator_e01_greedy.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/3_dumb_predator_e01_greedy_100.png}
	\end{minipage}
\end{figure}
%\error explain results missing

\subsection{Four Predators}
We tried to the fouth predator without changing the state representation and
it consistently caused the computer to run out of memory. So we implemented
the last state representation that was explained before. 

We also used the same evaluation model as the last section.

We also run this environment with $500000$ episodes because although the size of
the state space grows very fast we used a different representation for the
state that reduces is a lot. So turning it to be close to the size of the space
with three predators and the other state representation.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/4_max5.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/4_max5_100.png}
	\end{minipage}
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/4_minxy.png}
	\end{minipage}
	\begin{minipage}[t]{.45\linewidth}
		\centering
		\includegraphics[width=1\textwidth]{res/4_minxy_100.png}
	\end{minipage}
\end{figure}

As we can see, the figure shows that the values should converge and some
learning is happening. For the first one, we can see that the percentage of
caught converges to a higher value than the second one. We think that this is
because we loose more info about the direction. 
\section{MiniMax-Q}
In the article \emph{Markov games as a framework for multi-agent reinforcement learning} (Littman 1994)\footnote{Littman, M. L. (1994, July). Markov games as a framework for multi-agent reinforcement learning. In ICML (Vol. 94, pp. 157-163).}, Markov games are introduced as a way to model the interaction between agents in an MDP. The optimal policy in a zero-sum game is found by following the \emph{minimax} algorithm. The policy is learnt which maximizes the reward, given the fact that the opponent picks an action which is worst for you.

We implemented the linear program in the MiniMax-Q algorithm using a linear solver, the Apache Commons Math \emph{SimplexSolver}\footnote{Documentation of Apache Commons Math \emph{SimplexSolver}: http://commons.apache.org/proper/commons-math//apidocs/org/apache/commons/math3/optim/linear/SimplexSolver.html}.

The policy generated by the MiniMaxQ algorithm did not really fit our expectations. The recurring pattern is that $\pi(a_i) = 1.0$ for one of the actions $a_i$, and for all the other values $\forall a_j (a_j \neq a_i) \pi(a_j) = 0.0$. The policy does not converge to a best strategy given the worst case action of the opponent. We have not been able to explain this strange behaviour after extensive testing. 

\begin{figure}[!h]
\includegraphics[scale=0.5]{res/minimaxq-trip040.jpg}
\caption{Minimax-Q: $\gamma=0.5, \alpha = 0.5, \epsilon = 0.1$}
\label{fig:minimaxq}
\end{figure}

We expected a policy that decreases its number of steps drastically at the beginning, when $\alpha$ is high. Then, the policy converges slower, because $\alpha$ decays. Figure~\ref{fig:minimaxq} shows the results of minimax-Q for 10,000 episodes. It can be seen that the policy is not converging, but is almost a random process. Possible causes for this behaviour could be a wrong set-up of the linear solver or, less probable, a wrong update of the Q-values. We have however not been able to find the real cause for the problem. %When both the predator and the prey play an optimal action according to minimax, an episode will never converge. For this reason

The difference between indendent Q-learning and minimax-Q is that minimax-Q uses Markov games as its basis, while independent Q-learning does not. Because Markov games involve two players, only one predator can hunt one prey, while independent Q-learning can handle multiple predators. Minimax-Q could handle multiple predators, if a team of cooperating predators would be seen as one player in the Markov game.
\section{Conclusion}
The results of independent Q-learning look promising. The game-theoretic perspective of applying the minimax-algorithm is interesting, but did not turn out useful in our assignment, because we could not solve the error in our implementation.
\end{document}
