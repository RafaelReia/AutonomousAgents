\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 3: \emph{Multi-agent planning and learning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}

\section{Multi-predator random policy}
The first step for this assignment was to extend our environments to be able to
handle multiple agents. With more than one Predator and one Prey that can also
act as a learning agent.

We didn't change much of our implementation because most of our code was
already prepared to this changes. We runed some tests to use as a baseline for
the evaluation of the following evironments.

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|}
\hline
Number of predators    & 1       & 2       & 3       & 4      \\ \hline
Average steps of caugh & 217.144 & 80.977  & 45.248  & 31.203 \\ \hline
Standard Dev of caugh  & 199.497 & 63.250  & 34.917  & 18.432 \\ \hline
Average steps of clash &         & 41.535  & 13.659  & 5.082  \\ \hline
Standard Dev of clash  &         & 58.688  & 24.379  & 9.112  \\ \hline
\% Caught              & 100\%   & 44.40\% & 18.90\% & 5.40\% \\ \hline
\end{tabular}
\end{table}

Although, in this process we noticed that our state representation would grow
(exponentially) too much with the addition of more predators. Represented the
state space as the \emph{coordinates} of all individuals, which is implemented as
an array with $2 \times (n+1)$ dimensions, with the $n$ as the number of
predators. So with four predators we had a $10-D$ array with each dimension
with the value $11$.

To reduce the number of dimensions we implemented \emph{relative positions},
that represent the distance of each predator to the prey. With this we reduced the
state space by two dimensions. This doesn't look a lot at first sight but, it
makes the 3-predator situation solvable on our machines. And it also makes the
algorithm converge much faster on 1 or predator settings. Taking the 1-predator
as an example, with the original state-space, the algorithm will need to wait
until the action-value function converge on $121^2$ states; but now it only
need to converge on $121$ states.

To reduce even more the state space we represent the states as
\emph{(direction, square distance)} for each predator. With this we have the
same number of dimensions but the size of the direction dimension is reduced to $4$.
This have a significant impact on the size of the state space reducing it
by a factor of $4E4$. After that we reduce more the size of the state space with
two different options. The first was by setting the distance as $5$ if it was
higher that that and the second by seting the distance as the minimum of both
directions insetead of the sum.
With this we reduce the amount of information that we keep but we turn this into
a tractrable problem with four predators.

We can not implement the state as the \emph{square distance} to the prey that
would reduce the dimensions to just one for each predator. With this we would
loose important information for the agents. Direction is important to run away
when close.

\section{Multi-Agent independent Q-learning}
We implemented multi-agent independent Q-learning, and aplied to predators and
preys. With this, the prey does not have a random policy. They learn
independently of each other, this means that each agent is trying to optimize
their own rewards. So we have different Q-values sets and they get different
rewards.

\section{Multi-Agent independent Q-learning - Evaluation}
To evaluate our implementation of Multi-Agent independent Q-learning we tested
with one, two and three predators. We didn't tested with more because the state
space would be intractable with the available computing resources. In the
following subsections we will explain this three cases separately.

One big difference is between the case with only one predator and the cases with
more than one. First with only one predator, if the prey is smart enough it can
run away forever. To prevent that, the prey have a trip factor that causes the
prey to not be able to move a $\eta\%$ of times.

\subsection{One Predator}
As briefly explained, the behaviour for this settings is different from the
following ones. The predator always catch the prey, similar to the previous
assignments. So to evaluate this behaviour we use the number of steps needed to
catch the prey. With this we want to evaluate how the prey learns comparing to
the predator. Although it have a trip probability, the prey still would manage to
stay alive longer if it learns faster.

We run this environment with $1000$ episodes, we didn't use more because the
state space is small and is covered very fast and it converges very fast also.

The result the we have is in the following figures.

To test how our implementation evolves with a smart predator and a dumb prey we
used the set of parameters that resulted in the best values in the last
assignment for the predator, and really bad ones for the pray. We still want the
prey to learn so we didn't set any values to zero.

\error add picture, 1 predator, dumb prey

Here we don't see a big difference from the Q-Learning from the last assignment.
We have a smart predator and a dumb prey, so we expected this results.


After that, we tested a setting with both smart agents. So we used the best
values for the parameters for both agents. With this we wanted to show the effect
of the trip, because with the same parameters the agents learn at the same rate.
The only difference between the agents is the trip factor for the prey.

\error add picture, 1 predator, both smart, epsilon 0.1
\label{fig2}

The next step was to test the same settings as before but with a pure greedy
policy for both agents, that means, setting the $\epsilon$ to zero. Because we
have an optimistic initialisation, the agents still go to unexplored places
without the random move factor.

\error add picture, 1 predator, both smart, epsilon 0.0

With both agents learning fast, i.e. two smart agents, we can see on the last to
figures that it converges really fast but to a value sligtly higher than the
previous ones. This is because the prey is also learning.
We can see a difference between this to figures, the first with some random
moves ($\epsilon$ set to $0.1$) the first episodes have a smaller number of
steps than the other one. This happens becasue in the second case the agent
would always go to new states, exploring more in the first episodes and reduce
very fast the number of steps.
The spike in the figure \ref{fig2} can be explained by the following, the agent
found a set of states that wasn't explored yet.

After this we wanted to test if we can have a smart prey that manages to never
get caught and a dumb predator. Because we still have the trip
factor, we are just testing how long a prey manages to stay alive, and if this
number grows with learning.

\error add picture, 1 predator, dumb predator, smart prey

\error explain results missing


\subsection{Two Predators}
\label{twoP}
With more than one predator we add a new possible behaviour to the environment,
that is about the predators clashing into themselves and the prey runs away. The
probability of this behaviour to occur is very high at the beginning if the
predators star the close positions. To evaluate this new environment the number
of steps isn't enough because it doesn't take into account the occurrence of the
new behaviour, assuming that the prey is always caught. 

With this we have to compare the ratio between the number of times that the prey
is caught versus the number of times that escapes and from that together
with the number of steps we can have a much better understanding of what is
happening with our simulation.

We run this environment with $100000$ episodes,because the state space is grows
very fast with the number of individuals, we need to grow the number of episodes
accordingly to make sure that a higher rate of states are explored.

At first we wanted to test how agents with the same parameters, therefore
the same learning rates would behave in our environment. If we could reduce the
number of clashes, i.e. the predators learn to not kill each other and catch the
prey and how the prey would run away from a smart group of predators.

\error add picture, 2 predators, both smart, epsilon 0.1

As we can see, at the begining the predators clash a lot, so the ratio of
catch/clash is very low. But it looks like they ar learning to not kill each
other. After some time, the clashes almost goes to zero and then they start
optimizing the catches. We can clearly see that after this phase the number of
steps converges fast.


At second we tested two agents with the same parametes like before but the value
for $\epsilon$ is zero, so we are using a pure greedy policy.

\error add picture, 2 predators, both smart, epsilon 0.0

Again with two agents learning at the same rate, we changed the only the
$\epsilon$ from the last figure. We can see the same pattern than before with
just one predator. It first goes very high and then goes down very fast.

After that we tested how long a smart prey would live against less smart
predators, for that we used the same parameters from the last section but for
this we used again a pure greedy policy for the prey.
With this we test also if the clashing is a real problem and if it's being solved
trough learning. This would cause the predators to clash more with less
learning. 

\error add picture, 2 predators, dumb predator, smart prey

\error explain results missing


\subsection{Three Predators}
We add another predator to the environment explained in the section \ref{twoP}.
With more predators it should be easier and faster to catch the prey but on the
other hand the probability of clash increases. Then theoretically predators put
less effort to catch the prey and more to avoid clashes.

We also used the same evaluation model as the last section.

We run this environment with $500000$ episodes because as the size of the state
space grows very fast, the time that is needed to test this environments grows.
So we could not test with a bigger number of episodes because it would turn to
be impractical. However we still tested with higher number of episodes but not
in a systematic way.

Again we have the same testing goals as the last section, with both smart agents
we wanted to test how the prey would react with more predators and if they would
clash more or catch the prey faster.
\error add picture, 3 predators, both smart
\error explain results missing

After that we again tested with dumb predators and a smart pure greedy prey to
know how it would survive with one more predator.
\error add picture, 3 predators, both smart, epsilon 0.0
\error explain results missing

\subsection{Four Predators}
We tried to the fouth predator without changing the state representation and
it consistently caused the computer to run out of memory. So we implemented
the last state representation that was explained before. 

We also used the same evaluation model as the last section.

We also run this environment with $500000$ episodes because although the size of
the state space grows very fast we used a different representation for the
state that reduces is a lot. So turning it to be close to the size of the space
with three predators and the other state representation.

\error add picture, 4 predators, set to 5

\error add picture, 4 predators, set to min

As we can see, the figure shows that the values should converge and some
learning is happening. For the first one, we can see that the percentage of
caught converges to a higher value than the second one. We think that this is
because we loose more info about the direction. 
\section{MiniMax-Q}
In the article \emph{Markov games as a framework for multi-agent reinforcement learning} (Littman 1994)\footnote{Littman, M. L. (1994, July). Markov games as a framework for multi-agent reinforcement learning. In ICML (Vol. 94, pp. 157-163).}, Markov games are introduced as a way to model the interaction between agents in an MDP. The optimal policy in a zero-sum game is found by following the \emph{minimax} algorithm. The policy is learnt which maximizes the reward, given the fact that the opponent picks an action which is worst for you.

We implemented the linear program in the MiniMax-Q algorithm using a linear solver, the Apache Commons Math \emph{SimplexSolver}\footnote{Documentation of Apache Commons Math \emph{SimplexSolver}: http://commons.apache.org/proper/commons-math//apidocs/org/apache/commons/math3/optim/linear/SimplexSolver.html}.

The policy generated by the MiniMaxQ algorithm did not really fit our expectations. The recurring pattern is that $\pi(a_i) = 1.0$ for one of the actions $a_i$, and for all the other values $\forall a_j (a_j \neq a_i) \pi(a_j) = 0.0$. We have not been able to explain this strange behaviour after extensive testing. The policy does not converge to a best strategy given the worst case action of the opponent.

We expected a policy that decreases its number of steps drastically at the beginning, when $\alpha$ is high. Then, the policy converges slower, because $\alpha$ decays. %When both the predator and the prey play an optimal action according to minimax, an episode will never converge. For this reason
\section{Conclusion}

\end{document}
