\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 3: \emph{Multi-agent planning and learning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}

\section{Multi-predator random policy}
The first step for this assignment wast to extend our environments to be able to
handle multiple agents. With more than one Predator and one Prey that can also
act as a learning agent.

We didn't change much of our implementation because most of our code was
already prepared to this changes.

Altough, in this process we noticed that our state representation would grow
(exponentialy) too much with the addition of more predators. Represented the
state space as the \emph{coordenates} of all indiviuals, which is implemented as
an array with $2 \times (n+1)$ dimensions, with the $n$ as the number of predators. So with four predators we had a $10-D$ array with each
dimension with the value $11$. 

To reduce the number of dimensions we implemented \emph{relative positions},
that represent the distance of each predator to the prey. With this we reduced the
state space in two dimensions and reduced the size of each dimension in one.
This doesn't look a lot at first sight but, for instance with $4$ predators and
the prior representation we had $6.115909045E19$ states. With this
representation we have $1E7$ that is a huge reduction.

To reduce even more the state space we want to represent the states as
\emph{(direction, square distance)} for each predator. With this we have the
same number of dimensions but the size of the direction dimension is reduced to $4$.
This would have a significant impact on the size of the state space reducing it
by a factor of $4E4$.

We can not implement the state as the \emph{square distance} to the prey that
would reduce the dimensions to just one for each predator. With this we would
loose important information for the agents. Direction is importante to run away
when close.

\section{Multi-Agent independent Q-learning}
We implemented multi-agent independent Q-learning, and aplied to predators and
preys. With this, the prey does not have a random policy. They learn
independently of each other, this means that each agent is trying to optimize
their own rewards. So we have different Q-values sets and they get different
rewards.

\section{Multi-Agent independent Q-learning - Evaluation}
To evaluate our implementation of Multi-Agent independent Q-learning we tested
with one, two and three predators. We didn't tested with more becasue the state
space would be intractable with the available computing resources. In the
following subsections we will explain this three cases separately.

One big difference is between the case with only one predator and the cases with
more than one. First with only one predator, if the prey is smart enough it can
run away forever. To prevent that, the prey have a trip factor that causes the
prey to not be able to move a $\eta\%$ of times.

\subsection{One Predator}



\subsection{Two Predators}



\subsection{Three Predators}



\section{MiniMax-Q}
In the article \emph{Markov games as a framework for multi-agent reinforcement learning} (Littman 1994)\footnote{Littman, M. L. (1994, July). Markov games as a framework for multi-agent reinforcement learning. In ICML (Vol. 94, pp. 157-163).}, Markov games are introduced as a way to model the interaction between agents in an MDP. The optimal policy in a zero-sum game is found by following the \emph{minimax} algorithm. The policy is learnt which maximizes the reward, given the fact that the opponent picks an action which is worst for you.

We implemented the linear program in the MiniMax-Q algorithm using a linear solver, the Apache Commons Math \emph{SimplexSolver}\footnote{Documentation of Apache Commons Math \emph{SimplexSolver}: http://commons.apache.org/proper/commons-math//apidocs/org/apache/commons/math3/optim/linear/SimplexSolver.html}.

The policy generated by the MiniMaxQ algorithm did not really fit our expectations. The recurring pattern is that $\pi(a_i) = 1.0$ for one of the actions $a_i$, and for all the other values $\forall a_j (a_j \neq a_i) \pi(a_j) = 0.0$. We have not been able to explain this strange behaviour after extensive testing. The policy does not converge to a best strategy given the worst case action of the opponent.

We expected a policy that decreases its number of steps drastically at the beginning, when $\alpha$ is high. Then, the policy converges slower, because $\alpha$ decays. %When both the predator and the prey play an optimal action according to minimax, an episode will never converge. For this reason
\section{Conclusion}

\end{document}
