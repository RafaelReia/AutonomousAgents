\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\graphicspath{ {ReportImages/OneClassSVM/} }


\begin{document}

\title{Autonomous Agents\\
Assignment 3: \emph{Multi-agent planning and learning}}
\author{
Artur Alkaim -- 10859368\\
Peter Dekker -- 10820973\\
Rafael Reia -- 10859454\\
Yikang Wang -- 10540288\\
}
\maketitle
\section{Introduction}

\section{Multi-predator random policy}
The first step for this assignment was to extend our environments to be able to
handle multiple agents. With more than one Predator and one Prey that can also
act as a learning agent.

We didn't change much of our implementation because most of our code was
already prepared to this changes.

Altough, in this process we noticed that our state representation would grow
(exponentialy) too much with the addition of more predators. Represented the
state space as the \emph{coordenates} of all indiviuals, which is implemented as
an array with $2 \times (n+1)$ dimensions, with the $n$ as the number of predators. So with four predators we had a 10-D array with each
dimension with the value $11$. 

To reduce the number of dimensions we implemented \emph{relative positions},
that represent the distance of each predator to the prey. With this we reduced the
state space by two dimensions. This doesn't look a lot at first sight but, it makes the 3-predator situation solvable on our machines. And it also makes the algorithm converge much faster on 1 or predator settings. Taking the 1-predator as an example, with the original state-space, the algorithm will need to wait until the action-value function converge on $121^2$ states; but now it only need to converge on $121$ states.

To reduce even more the state space we want to represent the states as
\emph{(direction, square distance)} for each predator. With this we have the
same number of dimensions but the size of the direction dimension is reduced to $4$.
This would have a significant impact on the size of the state space reducing it
by a factor of $4E4$.

We can not implement the state as the \emph{square distance} to the prey that
would reduce the dimensions to just one for each predator. With this we would
loose important information for the agents. Direction is importante to run away
when close.

\section{Multi-Agent independent Q-learning}
In the article \emph{Markov games as a framework for multi-agent reinforcement learning} (Littman 1994)\footnote{Littman, M. L. (1994, July). Markov games as a framework for multi-agent reinforcement learning. In ICML (Vol. 94, pp. 157-163).}, Markov games are introduced as a way to model the interaction between agents in an MDP. The optimal policy is found by following the \emph{minimax} algorithm. The action chosen is the one which maximizes the reward, given the fact that the opponent picks an action which is worst for you.

We implemented multi-agent independent Q-learning, and aplied to predators and
preys. With this, the prey does not have a random policy. They learn
independently of each other, this means that each agent is trying to optimize
their own rewards. So we have different Q-values sets and they get different
rewards.

\section{Multi-Agent independent Q-learning - Evaluation}
To evaluate our implementation of Multi-Agent independent Q-learning we tested
with one, two and three predators. We didn't tested with more becasue the state
space would be intractable with the available computing resources. In the
following subsections we will explain this three cases separately.

One big difference is between the case with only one predator and the cases with
more than one. First with only one predator, if the prey is smart enough it can
run away forever. To prevent that, the prey have a trip factor that causes the
prey to not be able to move a $\eta\%$ of times.

\subsection{One Predator}



\subsection{Two Predators}



\subsection{Three Predators}



\section{MiniMaxQ}

\section{Conclusion}

\end{document}
